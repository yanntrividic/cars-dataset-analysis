---
title: "Cars dataset analysis"
author:
- Aissatou Signate
- Jacky Thay
- Yann Trividic
output:
  pdf_document: default
  html_document:
    df_print: paged
abstract: |
  This is the abstract.
  It consists of two paragraphs
tags:
- data analysis
- cars
- dataset
- R
- notebook
- pca
- clustering
- statistics
editor_options: 
  markdown: 
    wrap: sentence
---

# Sujet

## Sujet du projet

Ce projet est un travail sur données réelles.
Il s'agit de mettre en oeuvre une ou plusieurs méthodes vues en cours sur un jeu de données proposé pour chaque groupe.

Le projet doit suivre les étapes suivantes :

-   analyser le jeu de données proposé afin de choisir l'approche la plus adaptée à votre problème,
-   effectuer un prétraitement de votre base de données (si nécessaire),
-   justifier le choix de votre approche,
-   tester l'approche choisie sur l'ensemble de données en utilisant les options vues en cours,
-   analyser et commenter les résultats obtenus.

## Présentation du rapport

Le rapport du projet doit présenter de façon claire et concise:

-   l'objet de l'analyse,
-   la description des données (individus/variables utilisées, variables supplémentaires etc.),
-   l'analyse proprement dite,
-   les commentaires sur les résultats obtenus.

Ce rapport ne devrait pas dépasser 10 pages (les codes sources des programmes utilisés peuvent être mis en annexe).

Le projet sera jugé selon les critères suivants:

-   adéquation des méthodes utilisées aux données et problème étudiés,
-   richesse des analyses proposées (au delà du minimum requis),
-   justesse des commentaires sur les résultats,
-   qualité de la présentation du rapport.

## Remise du projet

Vous devez déposer votre rapport en format .pdf portant le nom : nom1_nom2_nom3_trinomeX.pdf au plus tard le 20 mai 2021 sous Moodle dans le dossier Rendu_projet_TND.

# Notes sur les différentes étapes

**1. analyser le jeu de données proposé afin de choisir l'approche la plus adaptée à votre problème**

-   Pour la **description des données**, tout ce qu'on a besoin de savoir est là : <https://www.kaggle.com/yugagrawal95/k-means-clustering-using-seaborn-visualization>. Décrire les différentes colonnes du jeu de données, les valeurs pouvant être prises par celles-ci, leur mode (numérique [entier, double, continue, discret], caractère, logique, `NULL`...), etc. Le nombre de colonnes aussi et la syntaxe comptent aussi. Quelles sont les données ordinales/qualitatives et quelles sont les données quantitatives.
-   Il faut **poser une problématique**, aussi générale soit-elle. Par exemple, quelle est l'influence des diverses variables sur les performances d'une voiture ? Comprendre quelles sont les avantages et les inconvénients d'avoir plus de chevaux, une plus grosse consommation d'essence, etc.
-   Combien a-t-on d'individus, combien de variables ?

**2. effectuer un prétraitement de votre base de données (si nécessaire)**

-   Effectuer une **analyse descriptive de base** (regarder ce qu'il est possible de déduire de la fonction `summary`, en faire un résumé, regarder ce qui ressort de la fonction `pairs`, et la fonction `quantile`, peut-être la matrice de corrélations ?)
-   Il y a-t-il des données ordinales ? [Non.] Textuelles ? [Oui, mais non, c'est pas des textes] Qualitatives ? [Oui.] Nominales ? [Oui.] Comment est-ce qu'on pourrait les traiter le mieux possible ? Les utiliser comme marqueurs dans la PCA et la CAH ? Par défaut, on mettra les variables qualitatives en tant que variables supplémentaires (ni l'ACP ni la CAH ne peuvent les traiter). Certaines variables quantitatives peuvent être considérées comme supplémentaires (par exemple, les variables qui décrivent d'autres variables ; le pH et le taux d'acide citrique)
-   Lister les variables qui sont le plus corrélées. On devrait retrouver ces corrélations dans les résultats de l'ACP
-   Potentiellement centrer les valeurs si les résultats de l'analyse univariée donne des résultats très hétérogènes. Càd si les valeurs prises par les variables sont sur des intervalles/variances/moyennes très différents.
-   Est-ce que tous les objets sont complets ? Il y a-t-il des **valeurs manquantes** (fonction `is.na`) ? Il y a-t-il des **valeurs aberrantes** à enlever (justifier pourquoi on les enlève) ? Décide-t-on de tolérer une marge d'erreur, remplacer les valeurs, supprimer les individus ? Essayer de comprendre pourquoi ces valeurs sont là, proposer des hypothèses si pertinent. Avec k-means par exemple, les valeurs aberrantes peuvent donner des résultats erronés. La forme des clusters est aussi importante.

**3. justifier le choix de votre approche**

-   Choisir un ensemble d'algorithmes à utiliser parmi ceux qu'on a vu (k-means, PCA, CAH, HCPC) et expliquer en quoi il serait pertinent de les utiliser au vu de ce qu'on a déduit du point précédent.
    Pour déterminer quel algo choisir, se référer aux caractéristiques des méthodes de classification (voir cours).

    -   Pourquoi l'ACP ? Trouver des individus similaires ; comment est-ce qu'une caractéristique va affecter les valeurs des autres caractéristiques, etc.

-   Avec chacun des algorithmes utilisés, justifier le choix la valeur de chacun des paramètres (exemple : pourquoi utiliser la méthode de Ward (voir cours), quels sont les résultats avec les autres méthodes ?
    Quel critère de qualité ?)

-   Ajouter l'utilisation d'autres d'outils statistiques : regression, chi², degré d'asymétrie (skewness), degré d'aplatissement (Kurtosis)...

**4. tester l'approche choisie sur l'ensemble de données en utilisant les options vues en cours**

-   Idéalement, je pense qu'il serait bien d'**enregistrer un fichier** `results.csv`, contenant les données de bases et une colonne pour chaque résultat pertinent.
-   Dans certains cas, il pourrait être intéressant de **diviser le jeu de données en différents sous-jeux** (par exemple, les voitures d'une certaine décennie, ou les voitures avec un certain type de moteur) et regarder comment les résultats varient au sein de ces sous-jeux. Pour ça, utiliser la fonction `aggregate`.
-   Consolider les résultats (consolider les résultats de la PCA avec HCPC, les résultats de la CAH avec k-means, etc.).

**5. analyser et commenter les résultats obtenus**

-   Se baser sur les corrections fournies pour les TD 5 et 6.
-   **Caractérisation des classes :** la partition obtenue peut être considérée comme une variable quantitative, chercher une variable qui caractérise le mieux une partition revient à chercher les variables qui caractérisent le mieux cette variable qualitative. Pour chaque variable quantitative, on va construire un modèle d'analyse de variance entre la variable quantitative, qui aura de le rôle de la variable réponse, en fonction de la variable de classe (qualitative). On construit ensuite un test de Fisher pour voir s'il y a un effet de la variable de classe sur la variable quantitative. On peut concerver les variables ayant une probabilité critique inférieure à 5 %, et trier ces variables par probabilité croissante. Attention : les variables ayant eu un rôle actif dans la construction des résultats (par exemple lorsqu'elles participent au calcul de la distance) ne sont pas totalement indépendantes des classes attribuées. Seules les probabilités critiques associées aux variables supplémentaires (dans notre cas, `brand`) peuvent être réellement interprétées comme décrit ici. Le test de Fisher nous permettrait donc de vérifier nos résultats de manière quantifiée et de savoir si les classes obtenues seraient représentatives ou non des classes obtenues avec les différents algorithmes de partitionnement.
-   On pourrait potentiellement colorer les résultats de l'ACP en fonction de certaines variables ayant beaucoup contribué dans les axes principaux.
-   Calculer quel est le nombre d'axe optimal à garder grâce au pourcentage d'inertie de l'ACP. Pareil pour la CAH.
-   Pour l'ACP, vérifier la qualité de la projection des différentes variables en regardant le cos² associé à cette variable.
-   Dans certains cas, un axe de l'ACP peut avoir été défini par un seul individu, ou une seule variable. Il peut être intéressant de refaire une ACP sans cette variable/individu si l'interprétation de ce phénomène le justifie.
-   Pour chaque interprétation, il est important de revenir aux données de départ pour les valider et citer des exemples.\

**2. Objet de l'analyse**

-   Expliciter des tendances sous-jacentes du jeu de données, analyse descriptive du jeu de données pour mieux comprendre les liens entre les différentes variables, et ainsi mieux comprendre les performances des véhicules en fonction de leurs attributs.

# Code

## 1. Lecture des données

Les données sont contenues dans un fichier `TXT`, les colonnes sont séparées par le caractère `,` et les lignes par un retour à la ligne.
Les valeurs décimales sont séparées des valeurs entières par le caractère `.`.

```{r read_file}
cars = read.table("../data/275-cars.txt", sep=",", header=T)
head(cars)
```

## 2. Descriptions du jeu de données

```{r dataset_dimensions}
dim(cars)
```

Le jeu de données cars contient 261 individus définis par 8 variables.
Chaque individu représente une liste de caractéristiques d'une voiture.
En se basant sur [ce lien](https://www.kaggle.com/abineshkumark/carsdata), on obtient les descriptions suivantes :

1.  `mpg` [numérique, réel, continu] : la quantité prédite de gallons par mille (continue, arrondie à l'unité)
2.  `cylinders` [numérique, entier, discret] : le nombre de cylindres dans le moteur. Peut être 3, 4, 6 ou 8.
3.  `cubicinches` [numérique, entier, continu] : mesure du volume du moteur de la voiture en pouces cube.
4.  `hp` [numérique, entier, continu] : puissance réelle du moteur en chevaux.
5.  `weightlbs` [numérique, entier, continu] : le poids de la voiture en livres.
6.  `time.to.60` [numérique, entier, continu] : durée nécessaire pour aller de 0 à 60 milles par heure.
7.  `year` [numérique, entier, discret] : année de fabrication de la voiture.
8.  `brand` [textuel, qualitatif, catégorielle] : région géographique de la marque de la voiture.

## 3. Prétraitement

```{r na_values}
cars[rowSums(is.na(cars)) > 0,] # counts the number of rows with NA values 
```


Le jeu de données ne contient aucune valeur manquante, il peut donc être utilisé tel quel pour la suite de l'analyse.

Seule la variable `brand` est une variable textuelle et catégorielle dans ce jeu de données.
Étant donné qu'il s'agit de la seule variable non quantitative, celle-ci peut être considérée comme une variable supplémentaire car la plupart algorithmes que nous utiliserons dans cette analyse demandent en entrée des tables de variables quantitatives.
`brand` nous servira donc principalement dans l'interprétation des résultats de ces algorithmes.
Nous pouvons par ailleurs changer son type pour le type `factor`, ce qui rendra les manipulations de cette variable plus simples.

Au premier abord, toutes les autres variables doivent être utilisés dans l'analyse, celles-ci ne semblant pas être liées entre elles directement.
On peut noter que les unités de mesure associées aux différentes variables sont toutes différentes ; le jeu de données sera analysé en prenant en compte ces caractéristiques.

```{r normalization}
cars$brand = as.factor(cars$brand) # Transformation en factor
cars.scaled = as.data.frame(cbind(scale(cars[-8]), cars$brand))
```

## 4. Analyse univariée

### 4.1 Sommaire, distributions et critères de position

#### 4.1.1 Variables quantitatives

Grâce à la fonction `summary`, nous avons un aperçu global des différentes distributions des variables composant notre base de données en calculant des statistiques de base (critères de position et critères de dispersion).
Pour les variables quantitatives, `summary` nous retourne le minimum, le maximum, la moyenne et les trois quartiles.
**[Yann : ajouter l'interprétation des résultats de summary, potentiellement le faire avec la fonction psych::describe pour être plus précis]**

```{r summary}
library(psych)
psych::describe(cars[-8])
summary(cars[-8]) # the last column is not included as it is a textual variable
```

Pour calculer des quantiles d'un jeu d'observations stocké dans un vecteur $v$, nous utilisons la fonction `quantile`.
Sans plus d'argument, celle-ci calcule les quantiles à 0 %, 25 %, 50 %, 75 % et 100 %.
Pour avoir les quantiles à d'autres ordres, il faut manipuler le paramètre `probs`.
Dans notre cas , nous les calculons par intervalle de 10 % :

```{r quantiles}
apply(cars[1:7], MARGIN=2, FUN=quantile, probs=seq(0, 1, 0.1))
```

Un box-plot est un graphique composé d'un rectangle, deux droites sortent afin de représenter certains éléments des données.

La valeur centrale du graphique est la médiane (il existe autant de valeur supérieures qu'inférieures à cette valeur dans l'échantillon).

Les bords du rectangle sont les quartiles (Pour le bord inférieur, un quart des observations ont des valeurs plus petites et trois quart ont des valeurs plus grandes, le bord supérieur suit le même raisonnement).

Les extrémités des moustaches sont calculées en utilisant 1.5 fois l'espace interquartile (la distance entre le 1er et le 3ème quartile).
On peut remarquer que 50% des observations se trouvent à l'intérieur de la boîte.
**[Yann : je suis pas sûr qu'il soit utile d'expliquer comment lire un boxplot, pour moi les quatre phrases qui précèdent ne sont pas utiles. A la limite, on pourrait mettre un lien vers la page Wiki si vraiment ça semble nécessaire]**

```{r boxplots}
par(mar=c(7,3,2,2))
boxplot(cars.scaled[-8], las=2, main="Diagrammes en boîte des variables quantitatives de cars")
```

**[Yann : aussi, j'pense que ce serait mieux si on avait en général les interprétations des graphiques après que les graphiques aient été affichés, du coup j'ai déplacé les graphiques]**

Ces diagrammes en boîtes permettent de visualiser plus facilement les résultats obtenus précédemment grâce à la fonction `summary`.
Par exemple, pour la variable `mpg`, on peut lire que sa valeur médiane est $22$, et que les valeurs sont un peu plus dispersées au-dessus de cette valeur médiane.
Pour la variable `cylinders`, on remarque une symétrie parfaite : la distribution en-dessous de la médiane est très similaire à celle au-dessus de celle-ci.
Son coefficient d'asymétrie est proche de $0$.
La même observation peut être faite concernant la variable `year`.

En ce qui concerne la variable `time.to.60`, on peut noter la présence de valeurs aberrantes.
Une valeur aberrante est une valeur qui s'écarte fortement des valeurs des autres observations, anormalement faible ou élevée.
Ici, ces valeurs correspondent à des voitures ayant une accélération anormalement élevée (\~8 secondes pour atteindre 60 mph pour la plus rapide) ou anormalement faible (\~25 secondes pour la plus lente).
Ces valeurs ne semblent pas être des erreurs de mesure mais simplement des voitures sortant de la norme.
Dans certains cas, il est nécessaire d'effectuer un traitement particulier sur ces valeurs.
Avec ces valeurs en particuliers, nous considérons qu'il est possible de continuer l'analyse en l'état, celles-ci n'allant pas poser pas de problèmes.

#### 4.1.2 Variables qualitatives

Le jeu de données `cars` contient, en plus des sept variables quantitatives déjà abordées, une variable qualitative : la variable `brand`.
Cette donnée, par sa nature, doit être traitée séparément du reste.

```{r piechart-brand}
library(ggplot2)
library(dplyr)

contingency_table <- table(cars$brand) 
df <- data.frame(brand = names(contingency_table),
                 Occurrences = as.vector(contingency_table))

ggplot(df, aes(x = "", y = Occurrences, fill = brand)) +
    geom_bar(width = 1, stat = "identity", color = "white") +
    coord_polar("y", start = 0)

prop.table(contingency_table)
```

Comme on peut le voir, la distribution des individus en fonction de la variable brand est tout sauf uniforme : près de deux tiers sont des voitures américaines (62 %) tandis que les voitures japonaises et européennes se partagent de manière à peu près égale le dernier tiers des données, soit respectivement environ 20 et 18 %.
Cette surréprésentation des voitures américaines sera à prendre en compte tout au long de l'analyse.

Il nous est aussi possible de résumer les différentes distributions des variables en fonction des valeurs de `brand`.

```{r variables-summary-by-brand}
lapply(split(cars[-8], cars$brand), summary)
lapply(split(cars[-8], cars$brand), psych::describe)
```

Le résultat obtenu nous permet d'avoir une idée claire des différences de distribution des valeurs en fonction de `brand`.
On peut voir notamment une hiérarchie entre les provenances des voiture et leur consommation en carburant : en règle générale, une voiture japonaise consommera moins qu'une européenne, qui elle-même consommera moins qu'une américaine.

D'autres interprétations peuvent être effectuées à partir de ces sommaires :

-   Les moteurs des voitures japonaises et européennes ont généralement moins de cylindres que ceux des américaines.
    La majorité des voitures européennes et japonaises auront quatre cylindres, tandis que l'écart-type est plus important pour les voitures amércaines ($\sigma=1,62$).

-   Le volume des moteurs des voitures américaines est en moyenne beaucoup plus important que celles des japonaises et des européennes.
    On peut noter une différence d'un facteur $2,5$.

-   Les voitures américaines sont beaucoup plus puissantes que les voitures européennes et japonaises en moyenne.

-   Les voitures américaines sont en moyenne beaucoup plus lourdes que les voitures européennes, qui sont elles-même légèrement plus lourdes que les voitures japonaises.

-   L'accélération des voitures américaines est globalement meilleure que celle des japonaise, qui elle-même est légèrement meilleure que celle des européennes.

De manière générale, on voit que les voitures européennes ont des caractéristiques proches des voitures japonaises.
Les voitures américaines sont quant à elles très différentes des deux autres catégories.

### 4.2 Corrélations

Une matrice de corrélation est utilisée pour visualiser la liaison linéaire entre plusieurs variables.
Il s'agit d'un tableau contenant les coefficients de corrélation entre chaque variable.
Pour l'obtenir, on peut utiliser la fonction `cor`, présente dans les fonctions de base de R.

```{r corr-mat}
cars.correlations <- cor(cars[-8]) # the last column is not included as it is a textual variable
#write.csv(cars.correlations, file="../data/correlations.csv")
cars.correlations
```

Nous pouvons également visualiser et représenter graphiquement notre matrice avec un corrélogramme.
Cet outil permet de mettre en évidence les variables les plus corrélées : les coefficients de corrélation sont colorés en fonction de leur valeur.

La valeur $1$ indique que les deux variables sont exactement corrélées, c'est le cas avec une relation parfaitement linéaire entre deux variables.
À l'inverse, une corrélation de $-1$ entre deux variables indique une parfaite anti-corrélation entre ces dernières.
Les corrélations dont la valeur absolue sont supérieures à $0,5$ peuvent être considérées comme des corrélations fortes.
Les autres, celles comprises entre $-0,5$ et $0,5$, peuvent être considérées comme des corrélations faibles.

Le 0.95 entre cylindre et cubinches signifie que le nombre de cylindres du véhicule (cubinches) joue pour 90,25% (0,95)\*(0,95) sur la valeur du volume moteur **[Yann : ce n'est pas juste, une corrélation n'implique pas une causalité]**

```{r corrplot}
library(corrplot)
corrplot(cars.correlations,addCoef.col = "grey", number.cex = 0.5)

#' Fonction permettant d'ordonner les différentes corrélations par ordre
#' décroissant par défaut en fonction de la valeur absolue de la corrélation
#'
#' @param df le dataframe dont les corrélations sont à extraire
#' @param order paramètre optionnel pour préciser l'ordre de tri (defaut:décroi)
#' @param threshold paramètre optionnel pour préciser un seuil (défaut: <=0.5)
#' @return un tableau contenant les corrélations ordonnées
getOrderedCorrelations <- function(df, order=T, threshold=0.5) {
    orderedCor <- as.data.frame(as.table(df))
    orderedCor <- orderedCor[order(abs(orderedCor$Freq), decreasing=order),]
    orderedCor <- orderedCor[abs(orderedCor$Freq)<1 & abs(orderedCor$Freq)>=threshold,]
    rownames(orderedCor) <- NULL
    withoutDuplicated <- orderedCor[seq(from=1, to=nrow(orderedCor), by=2),]
    rownames(withoutDuplicated) <- NULL
    return(withoutDuplicated)
}

orderedCor = getOrderedCorrelations(cars.correlations)
print(orderedCor)
```

À la lecture de ce corrélogramme, on peut noter que la variable `mpg` est très fortement anti-corrélée avec les variables `cylinders`, `cubicinches`, `hp` et `weightlbs`, avec des valeurs allant de $-0,78$ à $-0,82$.
De manière générale, on remarquera de très fortes corrélations positives entre les quatre dernières variables citées (toutes supérieures à $0,85$).
Ces observations peuvent s'interpréter de la manière suivante : plus un moteur est gros, plus il est puissant et la voiture est lourde, et inversement.
Plus la voiture est grosse et puissante, et plus sa consommation en carburant sera élevée.

Des corrélations existent aussi entre ces variables et la variable `time.to.60`, bien qu'elles soient moins importantes.
On pourra noter que plus une voiture a une consommation de carburant élevée et plus elle aura une forte accélération ; plus son moteur est gros et la voiture lourde, et plus ses capacités d'accélération seront importantes.

## 5. Classification

La section qui suit est basée sur le travail disponible à [ce lien](https://rstudio-pubs-static.s3.amazonaws.com/579984_6b9efbf84ee24f00985c29e24265d2ba.html).

### 5.1 Tendance à la classification

Avant d'utiliser quelconque algorithme de classification, il peut être intéressant de quantifier la propension du jeu de données à contenir des classes distinctes et pertinentes.
Cette quantification peut être effectuée grâce au [critère d'Hopkins](https://en.wikipedia.org/wiki/Hopkins_statistic).
Ce critère permet de mettre une valeur sur le degré d'uniformisation de la distribution des valeurs d'un jeu de données.
Si la valeur prise par le critère est proche de $0$, alors les distributions sont parfaitement uniformes.
Si la valeur approche $0,5$, alors distribution des valeurs est proche de celle d'une série générée par une loi de Poisson.
Les cas qui nous intéressent sont ceux où le critère approche $1$, cette valeur indiquant des classes très clairement définies.

```{r hopkins}
library(factoextra)
get_clust_tendency(cars[-8], 100, graph=FALSE)$hopkins_stat 
```

En utilisant la fonction `get_clust_tendency` de la bibliothèque `factoextra`, on remarque que la valeur prise par le critère de Hopkins pour le jeu de données cars est très forte.
On peut donc continuer le travail de classification.

### 5.2 K-moyennes

L'algorithme des K-moyennes, très simple à mettre en place et très utilisé dans le domaine de la classification, permet de distinguer des groupes d'individus en minimisant la somme totale des carrés des distance.
Cette méthode suppose cependant de préciser en paramètre le nombre de classes du jeu de données ; cependant, dans notre cas, l'information est manquante.

Notre jeu de données étant en sept dimensions (sept variables quantitatives), il est difficile de visualiser les résultats de l'algorithme des K-moyennes pour estimer le nombre de classes le plus approprié.
Afin de surmonter ce problème, il existe de nombreuses approches mathématiques pour quantifier la qualité des résultats de l'algorithme K-moyennes.
La plus connue est celle du [coefficient de silhouette](https://fr.wikipedia.org/wiki/Silhouette_(clustering)).
Nous l'utiliserons dans un premier temps pour obtenir des résultats préliminaires.

```{r silhouette}
fviz_nbclust(cars[-8], FUNcluster = kmeans, method = c("silhouette"), k.max = 15, nboot = 100)
```

Le coefficient de silhouette mesure la qualité de la partition obtenue après avoir appliqué un algorithme de classification.
Ici, on lance une série d'algorithmes K-moyennes avec le nombre de classe minimum à $1$ et le maximum à \$15\$.
On obtient les scores moyens obtenus par les différentes partitions en fonction du nombre de classes.

On remarque que la meilleure partition a obtenu un score d'environ $0,62$ (celui-ci pouvant monter jusqu'à \$1\$) avec deux classes.
Cependant, les scores obtenus par les partitionnements à trois et quatre classes sont relativement proche du meilleur score.
La différence entre ces scores n'est pas suffisante pour en garder un seul.

Il est donc préférable de garder les partitionnements à deux, à trois, et à quatre classes comme effectué ci-dessous.
Il est à noter que, bien que ce ne soit pas précisé explicitement, $10$ itérations de l'algorithme sont effectués pour chaque partitionnement afin de pallier à l'instabilité de K-moyennes à l'initialisation.

```{r k-means}
cars.kmeans.cluster2 <- kmeans(cars[-8], 2) # K-moyennes pour partitionnement à deux classes,
cars.kmeans.cluster3 <- kmeans(cars[-8], 3) # à trois classes,
cars.kmeans.cluster4 <- kmeans(cars[-8], 4) # à quatre classes.
```

Voici maintenant une visualisation de ces partitionnement couplés aux distributions normalisées des variables :

```{r stripplots-clusters}
library(lattice)
library(gridExtra)

#' Fonction permettant d'afficher une série de strip plots des variables d'un
#' data.frame et de les colorer et fonction de classes.
#' 
#' @param scaledQuantitativeValues data.frame ne comportant que des valeurs 
#' quantitatives normalisées
#' @param cluster classes associées aux individus du data.frame
stripplot_clusters <- function(scaledQuantitativeValues, cluster){
    v <- as.vector(unlist(cars.scaled[-8]))
    y <- rep(c(1:ncol(scaledQuantitativeValues)), each=nrow(scaledQuantitativeValues))
    xydata <- data.frame(v, y)
    colors = rep(cluster, ncol(scaledQuantitativeValues))
    xyplot(y ~ v, 
           xydata, 
           groups=colors,
           main = paste("Distribution des variables de cars colorées par", 
                        length(unique(cluster)),
                        "classes"),
           xlab = "Distributions",
           ylab = "Index des variables de cars",
           auto.key = list(space="top", x=0, y=0),
           lattice.options = list(legend.bbox="panel"))
}
       
stripplot_clusters(cars.scaled[-8], cars.kmeans.cluster2$cluster)
stripplot_clusters(cars.scaled[-8], cars.kmeans.cluster3$cluster)
stripplot_clusters(cars.scaled[-8], cars.kmeans.cluster4$cluster)
```

On peut voir sur ces différents graphiques les classes se séparer clairement.
On remarque que les variables corrélées voient la distributions de leurs classes être similaires, ce qui était prévisibles.
Dans la même idée, on observe que les distributions des classes sont en miroir lorsque qu'on regarde deux variables anti-corrélées.
Ces points sont tous cohérents avec les interprétations données dans la section *4.2 Corrélations*.
**[Vérifier section]**

Un point intéressant à noter est que la séparation des classes est plus prononcés pour certaines variables que pour d'autres.
Par exemple, la variable `weightlbs` (index 5) a dans les trois cas des classes parfaitement séparées.
Au contraire, `cubicinches` (index 1) voit la distribution de ses classes moins distincte.

En addition à cela, on peut comparer ces classes à la classe explicitement définie par le jeu de données : la variable `brand`.

```{r stripplots-brand}
stripplot_clusters(cars.scaled[-8], cars$brand)
```

Les voitures américaines prennent bien entendu la plus grande part des individus (voir section *4.1.2 Variables qualitatives* **[Vérifier section]**).
On remarque que les voitures américaines prennent presque systématiquement un côté du spectre des valeurs, tandis que les japonaises et les européennes se partagent la seconde extrémité de manière plus ou moins distincte selon les variables.

En regardant conjointement la distribution des variables colorées par le résultat de K-moyennes à trois classes et ce dernier graphique, on peut tenter d'assimiler les classes obtenues avec K-moyennes avec les valeurs de `brand` dans une table de contingence :

```{r contingency-table-brand-cluster3}
afficher_table_contingence_clusters = function(factor, cluster){
    cat("Valeurs absolues :","\n")
    print(rbind(table(factor, cluster), 
                tot=table(cluster)))
    
    cat("\n","Valeurs relatives :","\n")
    print(rbind(prop.table(table(factor, cluster)), 
                tot=prop.table(table(cluster)))*100)
}

afficher_table_contingence_clusters(cars$brand, cars.kmeans.cluster3$cluster)

```

**[TODO: attention, il semblerait que les classes puissent être inversées d'une exécution à l'autre... Ce qui semble logique, instabilité de K-moyennes ---\> classes à rectifier la suite de la section]**

En observant les résultat d'une table de contingence correspondant à cette hypothèse, cette dernière s'en voit invalidée ; les résultats ne soutiennent pas l'idée que les classes trouvées correspondraient de manière acceptable aux valeurs de `brand`.
En effet, la première classe contient autant de voiture américaines que japonaises ou européennes malgré le fait qu'elle soit la plus importante des trois classes (46 % des individus).

Cependant, il est à noter que cette même classe (la première) contient l'extrême majorité des voitures européennes et japonaises.
De plus, elle contient une minorité des voitures américaines.
Remarquons aussi que la troisième classe ne contient qu'une seule voiture non-américaine tandis qu'elle contient 61 américaines.
Une interprétation similaire, bien que moins prononcée, peut être faite sur la classe 2.
Les classes 2 et 3 caractérisent relativement bien les voitures américaines.

Il est donc intéressant maintenant de voir si, en groupant les voitures européennes et japonaises, on obtient un meilleur partitionnement avec $k = 2$.

```{r contingency-table-brand-cluster2}
cars.combined.brand <- cars$brand
levels(cars.combined.brand)[1:2] <- 'Non-US' # remplace Japon et Europe par Non-US

stripplot_clusters(cars.scaled[-8], cars.combined.brand)
afficher_table_contingence_clusters(cars.combined.brand, cars.kmeans.cluster2$cluster)
```

En comparant visuellement les résultats obtenus avec la fonction `stripplot_clusters` , on observe que même s'il y a une certaine correspondance entre les résultats obtenus, elle reste approximative.

Cette même observation peut-être faite en lisant la table de contingence.
On peut lire que plus de 95 % des voitures non américaines sont concentrées dans la classe 1.
Cependant, les 95 voitures non américaines de la classe 1 ne représente que 60 % du nombre de voitures totales de la classe 1.
En contrepartie, 60 % des voitures américaines sont dans la classe 2, et les 40 % restants étant dans la classe 1.

La tendance est assez claire : si une voiture est non américaine, il y a de grandes chances pour qu'elle soit dans la classe 1.
Cependant, les voitures américaines peuvent appartenir aux deux classes, bien que leur appartenance soit plus prononcées pour la classe 2.

En conclusion, l'interprétation des résultats de l'algorithme des K-moyennes permet donc de soutenir l'hypothèse qu'il existe bel et bien des différences quantitatives entre les différentes marques de voitures, et que celles-ci peuvent être partitionner au moins en partie.
Les différences les plus grandes sont entre les voitures américaines et les non américaines.
Les voitures européennes et japonaises sont différenciable, mais dans une moindre mesure.
Ces différences ne permettent cependant pas d'avoir un partitionnement clair et sans équivoque entre les marques à partir de l'algorithme des K-moyennes ; une part importante des voitures américaines est indifférenciée des voitures non américaines.
Il faut donc continuer l'analyse.

### 5.3 Classification Ascendante Hiérarchique (CAH)

#### 5.3.1 Critère de Ward

Avant de commencer à travailler sur le jeu de données avec l'algorithme de la classification ascendante hiérarchique, il est important de noter que le nombre d'individus du jeu de données (261) est conséquent pour cet algorithme.
Cela signifie que le dendrogramme résultant de la CAH comportera 261 feuilles ; la visualisation sera difficilement lisible et interprétable avec le dendrogramme seulement.

Une possibilité pour réduire le nombre d'individus serait d'appliquer un prétraitement comme K-moyennes avec par exemple $k = 50$, puis d'appliquer la CAH sur les parangons des cinquante centroïdes trouvés.
Ici, nous choisirons une autre approche pour interpréter les résultats, encore une fois basée sur la matrice de contingence entre `brand` et les classes obtenues.
Dans cette partie, nous utiliserons le critère de Ward par défaut, aucune raison ne permettant de prioriser un autre critère pour l'instant.
Le critère du lien minimum et celui du lien maximum sont utilisés dans la section suivante.

```{r hc-ward-inertia}
cars.dist = dist(cars.scaled[-8]) # matrice des distances euclidiennes entre individus

# method = « ward.D2 » correspond au vrai critère de Ward utilisant le carré de la distance
cars.hc.ward = hclust(cars.dist, method="ward.D2")
cars.hc.ward.inertia = sort(cars.hc.ward$height, decreasing = T)

plot(cars.hc.ward.inertia[1:20], type = "s", 
     xlab = "Nombre de classes", 
     ylab = "Inertie", 
     main = "Inertie du dendrogramme en fonction du nombre de classes")
```

Avant même de s'intéresser au dendrogramme en lui-même, il est intéressant de visualiser son inertie.
Ici, deux sauts se distinguent nettement.
Les deux variations d'inertie les plus grandes sont à 2 classes et à 4 classes :
**[Jack : je suppose que cah.ward.inertia = cars.hc.ward.inertia ? sinon le code ne se lançait pas]**

```{r hc-ward-inertia-critical-points}
plot(cars.hc.ward.inertia[1:20], type = "s",
     xlab = "Nombre de classes", 
     ylab = "Inertie", 
     main = "Inertie du dendrogramme avec plus grands gains entourés")
points(c(2, 4), cars.hc.ward.inertia[c(2, 4)], col = c("green", "red"), cex = 2, lwd = 3)
```

```{r hc-ward-partitions}
par(mar=c(1,4.5,2,0))
plot(cars.hc.ward, labels = FALSE, 
     main = "Partitions en 2 ou 4 classes", 
     ylab = "Hauteur", 
     hang = -1)
rect.hclust(cars.hc.ward, 2, border = "green")
rect.hclust(cars.hc.ward, 4, border = "red")
```

Encore une fois, comme avec K-moyennes, il pourrait être intéressant d'interpréter les résultats de la table de contingence entre `brand` et les classes résultant de la CAH, ce paramètre n'entrant pas en compte dans le calcul de la matrice de dissimilarité.
Pour ce faire, on utilise la fonction `afficher_table_contingence_clusters` :

```{r hc-ward-contingency-tables}
cut_affiche_table_contingence = function(hc_result, cut_vector, factor){
    for(cut in cut_vector){
        cat(paste("\nCut =", cut, "\n"))
        c = cutree(hc_result, cut)
        afficher_table_contingence_clusters(factor, c)
    }
}

# on coupe au niveau 4 et affichons les tables de contingences
cut_affiche_table_contingence(cars.hc.ward, c(2, 4), cars$brand) 
```

Pour le partionnement à deux classes, on peut observer que 100 % des voitures non américaines sont contenues dans la classe 2.
Cela constitue une différence majeure avec le partitionnement en deux classes obtenus avec K-moyennes, qui n'atteignait que 95 %.

La classe 2 comporte 54 % de voitures non américaines et 46 % de voiture américaines.
La première classe comporte 100 % de voitures américaines.
Le même ratio pour K-moyennes était de 60 contre 40 %.

Ces observations renforcent la validité des interprétations effectuées grâce aux résultats de K-moyennes : les voitures japonaises et européennes sont assez proches en ce qui concerne notre jeu de données, et dans l'ensemble assez différentes des voitures américaines.
Certaines voitures américaines sont cependant plus proches des voitures européennes et japonaises.

Pour le partitionnement à quatre classes, on remarque que les voitures européennes et japonaises sont chacune présentes dans les trois mêmes classes dans des proportions relativement proches.
Voici le résultat obtenu lorsque l'on décide de les regrouper :

```{r hc-ward-contingency-table-combined-brands}
cut_affiche_table_contingence(cars.hc.ward, 4, cars.combined.brand)
```

Comme avec le partionnement à deux classes, on observe qu'il existe une classe seulement constituées de voitures américaines (il s'agit rigoureusement de la même classe, voir le dendrogramme), et les autres constituées d'un mélange de voitures américaines et non américaines.
Cependant, à la différence du partitionnement précédent, les valeurs de `brand` sont plus finement séparées.

En effet, la classe 3 contient 78 % de voitures non américaines, et la classe 4 contient 85 % de voitures américaines.
Ces deux dernières classes caractérisent donc dans des proportions concluantes la différence entre les voitures américaines et non américaines.
La classe 2, quant à elle, contient 58 % de voitures non américaines contre 42 % de voitures américaines.
C'est dans cette classe que réside le plus d'incertitude quant à la détermination de la valeur de `brand`.
Cette classe compte pour 21,5 % du nombre d'individus total.

Le partitionnement à quatre classe est donc un meilleur partititionnement que celui à deux classes en ce qui concerne la catégorisation par marques.
Celui-ci est globalement plus fin pour différencier les voitures américaines des non américaines.
Ce partitionnement surclasse aussi de loin tous les résultats obtenus avec l'algorithme des K-moyennes.

```{r hc-ward-cluster4}
cars.hc.ward.cluster4 <- cutree(cars.hc.ward, 4)
```

Etant donné que seule la classe 2 présente des résultats peu concluant, il est intéressant de vérifier la structure de l'arbre qui la constitue pour peut-être la diviser en sous-classes.

On recommence donc le même procédé avec uniquement les individus de la classe 2 (**voir annexe**).
**[Yann : à mettre la partie ci-dessous en annexe]**
**[Jack : la ligne cars.hc.ward.cut4.tree2.ind <- as.numeric(names(cutree(cars.hc.ward.cut4.tree2, k=1))) me donne 'tree' incorrect (composante 'merge'), j'ai éssayé de corriger mais j'ai pas trouvé]**

```{r hc-ward-subclass}
# le sous-arbre de la classe 2 est extrait
cars.hc.ward.cut4.tree2 <- cut(as.dendrogram(cars.hc.ward), h = 13)$lower[[3]]
# puis on extrait les indices
cars.hc.ward.cut4.tree2.ind <- as.numeric(names(cutree(cars.hc.ward.cut4.tree2, k=1)))

dist = dist(cars.scaled[cars.hc.ward.cut4.tree2.ind, -8])
cah.ward = hclust(dist, method="ward.D2")

inertia = sort(cah.ward$height, decreasing = T)

plot(inertia[1:20], type = "s", 
     xlab = "Nombre de classes", 
     ylab = "Inertie", 
     main = "Inertie du dendrogramme en fonction du nombre de classes")

# les points d'intérêts sont 2, 3 et 4
plot(inertia[1:20], type = "s",
     xlab = "Nombre de classes", 
     ylab = "Inertie", 
     main = "Inertie du dendrogramme avec plus grands gains entourés")
points(c(2, 3, 4), inertia[c(2, 3, 4)], col = c("green", "red", "blue"), cex = 2, lwd = 3)

# affichage du sous-arbre
par(mar=c(1,4.5,2,0))
plot(cah.ward, labels = FALSE, main = "Partitions en 2, 3 ou 4 classes", xlab = "", ylab = "Hauteur", sub = "", hang = -1)
rect.hclust(cah.ward, 2, border = "green")
rect.hclust(cah.ward, 3, border = "red")
rect.hclust(cah.ward, 4, border = "blue")

# affichage des tables de contingence
cut_affiche_table_contingence(cah.ward, c(2, 3, 4), cars$brand[cars.hc.ward.cut4.tree2.ind])

# on remarque que le partitionnement en deux classes n'est pas suffisant, et que celui à quatre
# classes est trop. On remarque aussi que la distinction europe-japon est faible.

# combinaison des marques pour le partionnement en 3 classes :
afficher_table_contingence_clusters(cars.combined.brand[cars.hc.ward.cut4.tree2.ind], cah.ward.cut3)
```

Bien que le partionnement en trois sous-classes puisse affiner les résultats de la classification au sein de la classe 2, cet affinage est trop faible pour mériter l'ajout de trois nouvelles classes.

#### 5.3.2 Critères du lien minimum et du lien maximum

De la même manière, nous allons visualiser l'inertie avant le dendrogramme

```{r hc-single-link}
cars.hc.min = hclust(cars.dist, method="single")
par(mar=c(1,4.5,2,0))
plot(cars.hc.min, labels = FALSE, main = "Dendrogramme suite à la CAH avec lien minimum", xlab = "", ylab = "Hauteur", sub = "", hang = -1)
```

```{r hc-complete-link}
cars.hc.max = hclust(cars.dist, method="complete")
par(mar=c(1,4.5,2,0))
plot(cars.hc.max, labels = FALSE, main = "Dendrogramme suite à la CAH avec lien maximum", xlab = "", ylab = "Hauteur", sub = "", hang = -1)

cars.hc.max.inertia = sort(cars.hc.max$height, decreasing = T)

plot(cars.hc.max.inertia[1:20], type = "s", 
     xlab = "Nombre de classes", 
     ylab = "Inertie", 
     main = "Inertie du dendrogramme en fonction du nombre de classes")

# On remarque des points d'intérêt à 2, 3 et 4
plot(cars.hc.max.inertia[1:20], type = "s",
     xlab = "Nombre de classes", 
     ylab = "Inertie", 
     main = "Inertie du dendrogramme avec plus grands gains entourés")
points(c(2, 3, 4, 5), cars.hc.max.inertia[c(2, 3, 4, 5)], col = c("green", "red", "blue", "orange"), cex = 2, lwd = 3)

# affichage
par(mar=c(1,4.5,2,0))
plot(cars.hc.max, labels = FALSE, main = "Partitions en 2, 3, 4 ou 5 classes", xlab = "", ylab = "Hauteur", sub = "", hang = -1)
rect.hclust(cars.hc.max, 2, border = "green")
rect.hclust(cars.hc.max, 3, border = "red")
rect.hclust(cars.hc.max, 4, border = "blue")
rect.hclust(cars.hc.max, 5, border = "orange")

# affichage des tables de contingence
cut_affiche_table_contingence(cars.hc.max, c(2, 3, 4, 5), cars$brand)
# TODO: interpreter
```

## 6. Analyse en composantes principales pour interpréter les classes

Grâce à l'ACP, nous tenons compte des liaisons entre les variables.
**[TODO : c'est pas très précis comme définition, jusqu'à présent, tout ce qu'on a fait ou presque tient compte des liaisons entre les variables]**

```{r pca}
library("FactoMineR")
library("factoextra")
# paramètres à ajuster
#catdes(cars[, c(1:8)], 8)

# TODO: changer pour PCA
#cars.pca <- prcomp(cars[c(1:7)], center = TRUE,scale. = TRUE)
cars.pca <- PCA(cars.scaled[-8], scale.unit=F, ncp = 2, graph = F)
cars.pca
summary(cars.pca)

```

On obtient alors 7 composantes principales allant de `Dim1` à `Dim7`.

Dans ces résultats, la première composante `Dim1` présente une forte association avec l'ensemble des variables expceptés pour "year". Cette associativité est négative pour "mpg" (-89%) , "time.to.60" (-71%) et "year" (-48%)
[**TODO: ajouter des valeurs]**

Le tableau des valeurs propres est un outil permettant de déterminer quels sont les axes à prendre en compte pour l'ACP.
Il est important que les valeurs propres des axes retenus restituent une part importante de la variance totale du jeu de données.
Cela signifie que la somme de l'inertie expliquée par chacun des axes représente une partie importante de l'inertie totale.
C'est pour cette raison que nous allons retenir `Dim1` et `Dim2`, représentant respectivement 72 et 13 % d'inertie, pour un total de 85 % d'inertie.
On peut dire que ces deux axes restituent 85 % des informations du jeu de données d'origine.

Cette somme est une mesure de la fiabilité de la qualité globale explicative de l’analyse. **[TODO: définir lecture des mappings, perso je vois pas ce que tu veux dire]**

Nous allons réaliser une représentation graphique de notre ACP, nous utilserons un biplot pour cela.

```{r pca-plot}
cars.pca <- PCA(cars.scaled[-8], scale.unit=F, ncp = 2, graph = F)
fviz_pca_var(cars.pca, col.var = "black")

#cars: jeu de données de type data frame. Les lignes sont des individus et les colonnes sont des variables numériques
#scale.unit: une valeur logique. Si TRUE, les données sont standardisées/normalisées avant l’analyse.
#ncp: nombre de dimensions conservées dans les résultats finaux.
#graph: une valeur logique. Si TRUE un graphique est affiché.
```

**[TODO avec CAH : Réalisez une ACP sur les résultats obtenus à l'aide de la CAH. Coloriez les résultats de l'ACP en fonction d'une variable -\> Yann : pour colorer, prendre la variable cars.hc.ward.cluster4]**

Le graphique nous montre que sur les sept variables représentées, six sont corrélées avec `Dim1`, une observation cohérente puisque cette composante a une inertie de 72 % dans l'ensemble du data set.
Parmi les variables corrélées négativement avec `Dim2` nous trouvons `time.to.60` et `mpg`.
Elle mesure donc principalement le gabarit du véhicule et du moteur, ainsi que sa puissance
**[TODO : ce n'est pas vrai, c'est le contraire d'ailleurs. time.to.60 est la variable la moins corrélée avec Dim1 parmi les 6. A remplacer par un truc du genre "Elle mesure donc principalement le gabarit du véhicule et du moteur, ainsi que sa puissance"]**

Parmi les variables corrélées positivement avec `Dim1` nous avons les variables `hp` (95%), `cylinders` (93%), `cubicinches` (96%) et `weightlbs` (92%).
**[TODO: ajouter les valeurs de ces corrélations].**

Pour `Dim2`, l'unique variable qui lui est corrélée est `year`(86%), et elle est corrélée de façon positive. Dim2 caractérise alors l'âge du véhicule, nous pouvons également ajouter que l'âge est anti corrélé avec Dim2, ce qui implique que sur le graphique des individus, les voitures les plus vieilles seront plutôt en bas du graphique
[**TODO : pas si on regarde les résultats de prcomp - rmq prise en compte]**.

L'ensemble des variables se trouvent relativement loin de l'origine, elles sont donc bien représentées par l'ACP.
Nous pouvons vérifier cela en visualisant le carré du cosinus des variables sur toutes les dimensions en utilisant la bibliothèque `corrplot`.
**[TODO : j'ai l'impression que c'est juste une manière de tourner différemment tes interprétations précédentes à partir des mêmes données, je suis pas sûr que ce soit utile]**

```{r pca-representation-quality}
var <- get_pca_var(cars.pca)
#cosinus carrées des variables 
head(var$cos2, 4)
library("corrplot")
corrplot(var$cos2, is.corr=FALSE)

# Cos2 total des variables sur Dim.1 et Dim.2
fviz_cos2(cars.pca, choice = "var", axes = 1:2)
```

## 7. Classification hiérarchique sur composantes principales

```{r hcpc}
print(cars.pca)
cars.hcpc <- HCPC(cars.pca, nb.clust = -1, consol=T, iter.max=50, 
                  min = 3, 
                  metric = "euclidean", method = "ward", 
                  graph=T)

#cars.hcpc = HCPC(cars.pca)
#HCPC(cars, nb.clust = 0, graph = TRUE)
```

```{r hpcp}
fviz_dend(cars.hcpc, 
          cex = 0.7,                     # Taille du text
          palette = "jco",               # Palette de couleur ?ggpubr::ggpar
          rect = TRUE, rect_fill = TRUE, # Rectangle autour des groupes
          rect_border = "jco",           # Couleur du rectangle
          #labels_track_height = 0.8      # Augment l'espace pour le texte en 3D
          show_labels = FALSE            #Retire les labels
          )
# TODO: en l'état, on voit pas grand chose, y aurait p't'être moyen d'améliorer ça ? J'ai 
# cherché comment enlever les labels mais j'ai pas trouvé (un peu comme dans celui du
# chunk hc-ward-partitions)
```

Le dendogramme suggère une solution à 3 groupes, on peut également les visualiser sur l'ACP

```{r hpcp-viz}
fviz_cluster(cars.hcpc,
             repel = TRUE,            # Evite le chevauchement des textes
             show.clust.cent = TRUE, # Montre le centre des clusters
             palette = "jco",         # Palette de couleurs, voir ?ggpubr::ggpar
             ggtheme = theme_minimal(),
             main = "Factor map"
             )
```

```{r hcpc-interpretation-description-par-les-PC-/-axex-principaux }

cars.hcpc$desc.axes$quanti
```
Les résultats ci-dessus indiquent que les individus dans les groupes 1 et 3 ont des coordonnées élevées sur l’axe 1 (Dim1) et l'axe 2(Dim2) .
Les individus du groupe 2 ont des coordonnées élevées uniquement sur le deuxième axe.


```{r hcpc-description-par-les-individus}
cars.hcpc$desc.ind
```
Pour chaque groupe, les 5 meilleurs individus les plus proches du centre du cluster sont affichés et sont appelés paragones. La distance entre chaque individu et le centre du groupe est fournie. Par exemple, les individus représentatifs pour le groupe 2 inclus les individus/véhicules suivants : 87, 34, 229, 208 et 159 
